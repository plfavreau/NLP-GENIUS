{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRyZzcTPUQ1O"
      },
      "source": [
        "Ce notebook est dédié au classifier (Lyrics to genre) du projet NLP.\n",
        "Commençons par importer les différents packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iVWgPQFOHgW",
        "outputId": "c1742934-afbe-4428-b6d8-9a3267354792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dcAYt-NFI28R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "#from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDdVUSw_UgJF"
      },
      "source": [
        "Ensuite vous aurez besoin de récupérer le dataset en provenance de kaggle.\n",
        "Pour se faire, je vous prie de suivre ce tuto\n",
        "https://www.kaggle.com/discussions/general/74235#2580958\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDC8kI5JtYRV",
        "outputId": "59c4da34-d196-4db1-cec1-ce01da2c8cd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading genius-song-lyrics-with-language-information.zip to /content\n",
            "100% 3.04G/3.04G [01:18<00:00, 41.6MB/s]\n",
            "100% 3.04G/3.04G [01:18<00:00, 41.7MB/s]\n",
            "Archive:  genius-song-lyrics-with-language-information.zip\n",
            "  inflating: song_lyrics.csv         \n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "!kaggle datasets download -d carlosgdcj/genius-song-lyrics-with-language-information\n",
        "!unzip genius-song-lyrics-with-language-information.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHPqMCKVK_7"
      },
      "source": [
        "Etape de préprocessing 'rapide'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5FMy0qwI5IF",
        "outputId": "f8be2aa2-e663-441f-e11d-c05bf58a01cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RangeIndex(start=0, stop=10269, step=1)\n"
          ]
        }
      ],
      "source": [
        "# n = 100 every 100th line = 1% of the lines 50 000 lines taken\n",
        "df = pd.read_csv(\"../song_lyrics.csv\", skiprows=lambda i: i % 500 != 0)\n",
        "print(df.index)\n",
        "df = df[df['tag'] != 'misc']\n",
        "if 'language' in df.columns:\n",
        "    df = df[df['language'] == 'en']\n",
        "df = df[['title', 'lyrics', 'tag']]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "# To shuffle randomnly datas\n",
        "df = df.sample(frac = 1)\n",
        "# Split the data into features (X) and labels (Y)\n",
        "X = df['lyrics']\n",
        "Y = df['tag']\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtysXMjoVWoE"
      },
      "source": [
        "Utilisons les tokenizers vu en cours encore une fois dans un soucis de rapidité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lK8H-lVxa2k_"
      },
      "outputs": [],
      "source": [
        "#Define tokenizers from course\n",
        "def lemma_tokenize(doc):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    return [wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "def char_tokenize(doc):\n",
        "    return [char for char in doc]\n",
        "\n",
        "def byte_tokenize(doc):\n",
        "    tokens = doc.encode(\"utf-8\")\n",
        "    tokens = list(map(int, tokens))\n",
        "    return [str(token) for token in tokens]\n",
        "\n",
        "def gpt_tokenize(doc):\n",
        "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    tokens = enc.encode(doc)\n",
        "    return [str(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOYfNa1Venh"
      },
      "source": [
        "Création du classifier en utilisant scikit-learn\n",
        "On peut s'amuser à jouer sur les différents paramètres et hyperparamètres pour voir si on obtient de meilleur résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "k6WEHzZM3F3R"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'MultinomialNB' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[96], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create model, we can test them one by one or even customize them using hyperparameters tunning\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m make_pipeline(CountVectorizer(ngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), stop_words \u001b[38;5;241m=\u001b[39m en_stop), \u001b[43mMultinomialNB\u001b[49m()) \u001b[38;5;66;03m#Naive Bayes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model = make_pipeline(CountVectorizer(ngram_range = (1,1), stop_words = en_stop), LogisticRegression()) #Logistic Regression\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
          ]
        }
      ],
      "source": [
        "# Create model, we can test them one by one or even customize them using hyperparameters tunning\n",
        "model = make_pipeline(CountVectorizer(ngram_range = (1,1), stop_words = en_stop), MultinomialNB()) #Naive Bayes\n",
        "#model = make_pipeline(CountVectorizer(ngram_range = (1,1), stop_words = en_stop), LogisticRegression()) #Logistic Regression\n",
        "model.fit(X_train, Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rY8evECVyx6"
      },
      "source": [
        "On évalue dans un premier temps le modèle simplement en regardant sa précision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc620KYbVxLz"
      },
      "outputs": [],
      "source": [
        "accuracy = model.score(X_test, Y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfcIoe5mXJaC"
      },
      "source": [
        "Ce-dessous une liste de toutes les combinaisons qui ont été testées une par une. Pour obtenir une vue des résultats : voir l'annexe dans le repo github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFJtJ4OeWEg6"
      },
      "outputs": [],
      "source": [
        "#model = make_pipeline(CountVectorizer(tokenizer=gpt_tokenize, ngram_range=(1, 1)), scaler, LogisticRegression( max_iter = 1000, solver='saga',penalty='l2'))\n",
        "#model = make_pipeline(CountVectorizer(tokenizer=byte_tokenize, ngram_range=(1, 1)), scaler, LogisticRegression( max_iter = 1000, solver='saga',penalty='l2'))\n",
        "#model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 1)), scaler, LogisticRegression( max_iter = 1000, solver='saga',penalty='l2'))\n",
        "#model = make_pipeline(CountVectorizer(ngram_range=(1, 1), scaler, LogisticRegression( max_iter = 3000, solver='lbfgs'))\n",
        "#model = make_pipeline(CountVectorizer(ngram_range=(1, 2), scaler, LogisticRegression( max_iter = 3000, solver='lbfgs'))\n",
        "\n",
        "\n",
        "#model = make_pipeline(CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 1)), scaler, LogisticRegression( max_iter = 1000, solver='saga',penalty='elasticnet', l1-ratio=0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETSGehw5YplL"
      },
      "source": [
        "Voici comment évaluer le modèle de manière plus pertinente.\n",
        "On se permet d'utiliser\n",
        "\n",
        "*   La matrice de confusion\n",
        "*   Le classification report fourni par sk-learn\n",
        "*   L'évaluation empirique\n",
        "\n",
        "Matrice de confusion : on voit précisément où le modèle s'est trompé / où il a bien prédit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKtlyCMRZdN0"
      },
      "outputs": [],
      "source": [
        "# Print classification report and confusion matrix\n",
        "print(\"Classification Report:\\n\", classification_report(Y_test, Y))\n",
        "cm = confusion_matrix(Y_test, Y)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ7g8ojeZnuF"
      },
      "source": [
        "Classification report : qu'on envoit vers un fichier excel pour ensuite le comparer avec d'autres itérations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo1B4wR_ZlfV"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(Y_test, y_pred))\n",
        "# Convert classification report to dictionary\n",
        "report_dict = classification_report(Y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_excel = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "# Convert the DataFrame to Excel format\n",
        "df_excel.to_excel(\"classification_report.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBfC3e_ZuZT"
      },
      "source": [
        "Tests empirique 'visuels' : on prend au hasard certaines musiques.\n",
        "Et on regarde si notre modèle performe bien dessus ou pas.\n",
        "Enfin on affiche les erreurs/réussites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOMoRncdYp_0"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\"song_lyrics.csv\", skiprows=lambda i: i % 977 != 0 , nrows=10) # Change here to test different values\n",
        "\n",
        "df_test = df_test[df_test['tag'] != 'misc']\n",
        "if 'language' in df_test.columns:\n",
        "    df_test = df_test[df_test['language'] == 'en']\n",
        "df_test = df_test[['title', 'lyrics', 'tag']]\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for song_name, song_lyrics, song_tag in zip(df_test['title'], df_test['lyrics'], df_test['tag']):\n",
        "    print(\"Song:\", song_name)\n",
        "    print(\"Tag:\", song_tag)\n",
        "    # Convert the lyrics to a list and predict probabilities\n",
        "    probabilities = model.predict_proba([song_lyrics])\n",
        "\n",
        "    # Print the distribution of probabilities\n",
        "    print(\"Distribution of Probabilities:\")\n",
        "    for class_label, probability in zip(model.classes_, probabilities[0]):\n",
        "        if(probability > 0.0001):\n",
        "          print(f\"{class_label}: {probability:.4f}\")\n",
        "    max_prob_index = probabilities.argmax()\n",
        "    predicted_class = model.classes_[max_prob_index]\n",
        "    if predicted_class != song_tag:\n",
        "        print(f'Model failed to predict. Actual tag is {song_tag}, predicted tag is {predicted_class}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGihQ2xWV3DO"
      },
      "source": [
        "Dans une optique de performance nous avons décidé de voir ce si les perfomances étaient meilleures sans utiliser scikit-learn\n",
        "\n",
        "Passons donc maintenant à l'implementation du classifer à l'aide de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TjxcYIZs1gVF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "liEEQOpb1A0E"
      },
      "outputs": [],
      "source": [
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "Y_train_vec = vectorizer.fit_transform(Y_train)\n",
        "Y_test_vec = vectorizer.transform(Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "JNxBk6Bk1CcO"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_vec.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_vec.toarray(), dtype=torch.float32)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "Y_train_indices = label_encoder.fit_transform(Y_train)\n",
        "Y_test_indices = label_encoder.transform(Y_test)\n",
        "\n",
        "Y_train_tensor = torch.tensor(Y_train_indices, dtype=torch.long)\n",
        "Y_test_tensor = torch.tensor(Y_test_indices, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "YTDiq8kEplVV"
      },
      "outputs": [],
      "source": [
        "# Define logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        #self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        #out = self.relu(out)\n",
        "        out = self.softmax(out)\n",
        "        #out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "input_size = X_train_tensor.shape[1]\n",
        "num_classes = len(Y_train.unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DuddfuGYUnI"
      },
      "source": [
        "On utilise la CrossEntropy Loss et l'optimizer Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "6U07jIJ61J8n"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = LogisticRegression(input_size, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8z60Vy-73-h",
        "outputId": "f8dd7fbe-05a8-45a1-e927-ad5fab4e156c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5096, 35132])\n",
            "torch.Size([1275, 35132])\n",
            "torch.Size([5096])\n"
          ]
        }
      ],
      "source": [
        "print(X_train_tensor.shape)\n",
        "print(X_test_tensor.shape)\n",
        "print(Y_train_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wHL-3QqW1Nhv",
        "outputId": "db6ca0ea-9a0a-4973-de7a-978dcf3dd9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Loss: 1.6071, Accuracy: 30.35%\n",
            "Epoch [2/200], Loss: 1.6042, Accuracy: 34.35%\n",
            "Epoch [3/200], Loss: 1.6014, Accuracy: 36.47%\n",
            "Epoch [4/200], Loss: 1.5986, Accuracy: 38.82%\n",
            "Epoch [5/200], Loss: 1.5959, Accuracy: 40.24%\n",
            "Epoch [6/200], Loss: 1.5933, Accuracy: 41.10%\n",
            "Epoch [7/200], Loss: 1.5906, Accuracy: 42.43%\n",
            "Epoch [8/200], Loss: 1.5880, Accuracy: 43.53%\n",
            "Epoch [9/200], Loss: 1.5854, Accuracy: 44.47%\n",
            "Epoch [10/200], Loss: 1.5828, Accuracy: 44.47%\n",
            "Epoch [11/200], Loss: 1.5802, Accuracy: 44.86%\n",
            "Epoch [12/200], Loss: 1.5775, Accuracy: 45.18%\n",
            "Epoch [13/200], Loss: 1.5749, Accuracy: 45.25%\n",
            "Epoch [14/200], Loss: 1.5722, Accuracy: 45.49%\n",
            "Epoch [15/200], Loss: 1.5695, Accuracy: 45.41%\n",
            "Epoch [16/200], Loss: 1.5668, Accuracy: 45.73%\n",
            "Epoch [17/200], Loss: 1.5641, Accuracy: 45.73%\n",
            "Epoch [18/200], Loss: 1.5614, Accuracy: 45.88%\n",
            "Epoch [19/200], Loss: 1.5587, Accuracy: 45.88%\n",
            "Epoch [20/200], Loss: 1.5559, Accuracy: 46.12%\n",
            "Epoch [21/200], Loss: 1.5532, Accuracy: 45.88%\n",
            "Epoch [22/200], Loss: 1.5504, Accuracy: 45.80%\n",
            "Epoch [23/200], Loss: 1.5477, Accuracy: 45.49%\n",
            "Epoch [24/200], Loss: 1.5449, Accuracy: 45.49%\n",
            "Epoch [25/200], Loss: 1.5422, Accuracy: 45.33%\n",
            "Epoch [26/200], Loss: 1.5394, Accuracy: 45.25%\n",
            "Epoch [27/200], Loss: 1.5367, Accuracy: 45.49%\n",
            "Epoch [28/200], Loss: 1.5339, Accuracy: 45.10%\n",
            "Epoch [29/200], Loss: 1.5312, Accuracy: 45.10%\n",
            "Epoch [30/200], Loss: 1.5285, Accuracy: 45.25%\n",
            "Epoch [31/200], Loss: 1.5258, Accuracy: 45.41%\n",
            "Epoch [32/200], Loss: 1.5232, Accuracy: 45.41%\n",
            "Epoch [33/200], Loss: 1.5205, Accuracy: 45.80%\n",
            "Epoch [34/200], Loss: 1.5179, Accuracy: 45.73%\n",
            "Epoch [35/200], Loss: 1.5153, Accuracy: 46.04%\n",
            "Epoch [36/200], Loss: 1.5127, Accuracy: 45.73%\n",
            "Epoch [37/200], Loss: 1.5102, Accuracy: 45.88%\n",
            "Epoch [38/200], Loss: 1.5077, Accuracy: 46.04%\n",
            "Epoch [39/200], Loss: 1.5052, Accuracy: 46.20%\n",
            "Epoch [40/200], Loss: 1.5027, Accuracy: 46.51%\n",
            "Epoch [41/200], Loss: 1.5002, Accuracy: 46.82%\n",
            "Epoch [42/200], Loss: 1.4978, Accuracy: 47.22%\n",
            "Epoch [43/200], Loss: 1.4954, Accuracy: 47.92%\n",
            "Epoch [44/200], Loss: 1.4931, Accuracy: 48.55%\n",
            "Epoch [45/200], Loss: 1.4907, Accuracy: 48.86%\n",
            "Epoch [46/200], Loss: 1.4884, Accuracy: 49.49%\n",
            "Epoch [47/200], Loss: 1.4861, Accuracy: 49.80%\n",
            "Epoch [48/200], Loss: 1.4838, Accuracy: 50.43%\n",
            "Epoch [49/200], Loss: 1.4815, Accuracy: 50.90%\n",
            "Epoch [50/200], Loss: 1.4793, Accuracy: 51.22%\n",
            "Epoch [51/200], Loss: 1.4771, Accuracy: 51.76%\n",
            "Epoch [52/200], Loss: 1.4749, Accuracy: 52.86%\n",
            "Epoch [53/200], Loss: 1.4727, Accuracy: 53.49%\n",
            "Epoch [54/200], Loss: 1.4706, Accuracy: 54.20%\n",
            "Epoch [55/200], Loss: 1.4684, Accuracy: 54.59%\n",
            "Epoch [56/200], Loss: 1.4663, Accuracy: 55.06%\n",
            "Epoch [57/200], Loss: 1.4642, Accuracy: 55.84%\n",
            "Epoch [58/200], Loss: 1.4621, Accuracy: 56.31%\n",
            "Epoch [59/200], Loss: 1.4600, Accuracy: 56.63%\n",
            "Epoch [60/200], Loss: 1.4580, Accuracy: 57.57%\n",
            "Epoch [61/200], Loss: 1.4559, Accuracy: 57.73%\n",
            "Epoch [62/200], Loss: 1.4539, Accuracy: 57.73%\n",
            "Epoch [63/200], Loss: 1.4519, Accuracy: 58.12%\n",
            "Epoch [64/200], Loss: 1.4499, Accuracy: 58.27%\n",
            "Epoch [65/200], Loss: 1.4480, Accuracy: 58.67%\n",
            "Epoch [66/200], Loss: 1.4460, Accuracy: 59.29%\n",
            "Epoch [67/200], Loss: 1.4441, Accuracy: 59.61%\n",
            "Epoch [68/200], Loss: 1.4422, Accuracy: 59.76%\n",
            "Epoch [69/200], Loss: 1.4403, Accuracy: 60.00%\n",
            "Epoch [70/200], Loss: 1.4384, Accuracy: 60.08%\n",
            "Epoch [71/200], Loss: 1.4365, Accuracy: 60.00%\n",
            "Epoch [72/200], Loss: 1.4346, Accuracy: 60.24%\n",
            "Epoch [73/200], Loss: 1.4328, Accuracy: 60.47%\n",
            "Epoch [74/200], Loss: 1.4310, Accuracy: 60.55%\n",
            "Epoch [75/200], Loss: 1.4292, Accuracy: 60.78%\n",
            "Epoch [76/200], Loss: 1.4274, Accuracy: 60.86%\n",
            "Epoch [77/200], Loss: 1.4256, Accuracy: 61.18%\n",
            "Epoch [78/200], Loss: 1.4239, Accuracy: 61.25%\n",
            "Epoch [79/200], Loss: 1.4221, Accuracy: 61.18%\n",
            "Epoch [80/200], Loss: 1.4204, Accuracy: 61.10%\n",
            "Epoch [81/200], Loss: 1.4187, Accuracy: 61.18%\n",
            "Epoch [82/200], Loss: 1.4170, Accuracy: 61.49%\n",
            "Epoch [83/200], Loss: 1.4153, Accuracy: 61.49%\n",
            "Epoch [84/200], Loss: 1.4136, Accuracy: 61.49%\n",
            "Epoch [85/200], Loss: 1.4120, Accuracy: 61.33%\n",
            "Epoch [86/200], Loss: 1.4103, Accuracy: 61.33%\n",
            "Epoch [87/200], Loss: 1.4087, Accuracy: 61.41%\n",
            "Epoch [88/200], Loss: 1.4071, Accuracy: 61.49%\n",
            "Epoch [89/200], Loss: 1.4055, Accuracy: 61.49%\n",
            "Epoch [90/200], Loss: 1.4039, Accuracy: 61.57%\n",
            "Epoch [91/200], Loss: 1.4023, Accuracy: 61.57%\n",
            "Epoch [92/200], Loss: 1.4008, Accuracy: 61.73%\n",
            "Epoch [93/200], Loss: 1.3992, Accuracy: 61.65%\n",
            "Epoch [94/200], Loss: 1.3977, Accuracy: 61.57%\n",
            "Epoch [95/200], Loss: 1.3962, Accuracy: 61.65%\n",
            "Epoch [96/200], Loss: 1.3947, Accuracy: 61.65%\n",
            "Epoch [97/200], Loss: 1.3932, Accuracy: 61.57%\n",
            "Epoch [98/200], Loss: 1.3917, Accuracy: 61.57%\n",
            "Epoch [99/200], Loss: 1.3902, Accuracy: 61.65%\n",
            "Epoch [100/200], Loss: 1.3888, Accuracy: 61.65%\n",
            "Epoch [101/200], Loss: 1.3873, Accuracy: 61.65%\n",
            "Epoch [102/200], Loss: 1.3859, Accuracy: 61.73%\n",
            "Epoch [103/200], Loss: 1.3845, Accuracy: 61.73%\n",
            "Epoch [104/200], Loss: 1.3831, Accuracy: 61.65%\n",
            "Epoch [105/200], Loss: 1.3817, Accuracy: 61.65%\n",
            "Epoch [106/200], Loss: 1.3803, Accuracy: 61.49%\n",
            "Epoch [107/200], Loss: 1.3789, Accuracy: 61.49%\n",
            "Epoch [108/200], Loss: 1.3775, Accuracy: 61.49%\n",
            "Epoch [109/200], Loss: 1.3762, Accuracy: 61.49%\n",
            "Epoch [110/200], Loss: 1.3748, Accuracy: 61.49%\n",
            "Epoch [111/200], Loss: 1.3735, Accuracy: 61.57%\n",
            "Epoch [112/200], Loss: 1.3722, Accuracy: 61.57%\n",
            "Epoch [113/200], Loss: 1.3708, Accuracy: 61.57%\n",
            "Epoch [114/200], Loss: 1.3695, Accuracy: 61.49%\n",
            "Epoch [115/200], Loss: 1.3682, Accuracy: 61.41%\n",
            "Epoch [116/200], Loss: 1.3669, Accuracy: 61.57%\n",
            "Epoch [117/200], Loss: 1.3657, Accuracy: 61.65%\n",
            "Epoch [118/200], Loss: 1.3644, Accuracy: 61.65%\n",
            "Epoch [119/200], Loss: 1.3631, Accuracy: 61.65%\n",
            "Epoch [120/200], Loss: 1.3619, Accuracy: 61.57%\n",
            "Epoch [121/200], Loss: 1.3606, Accuracy: 61.57%\n",
            "Epoch [122/200], Loss: 1.3594, Accuracy: 61.65%\n",
            "Epoch [123/200], Loss: 1.3582, Accuracy: 61.73%\n",
            "Epoch [124/200], Loss: 1.3570, Accuracy: 61.65%\n",
            "Epoch [125/200], Loss: 1.3558, Accuracy: 61.73%\n",
            "Epoch [126/200], Loss: 1.3546, Accuracy: 61.73%\n",
            "Epoch [127/200], Loss: 1.3534, Accuracy: 61.73%\n",
            "Epoch [128/200], Loss: 1.3522, Accuracy: 61.73%\n",
            "Epoch [129/200], Loss: 1.3510, Accuracy: 61.49%\n",
            "Epoch [130/200], Loss: 1.3498, Accuracy: 61.57%\n",
            "Epoch [131/200], Loss: 1.3487, Accuracy: 61.65%\n",
            "Epoch [132/200], Loss: 1.3475, Accuracy: 61.65%\n",
            "Epoch [133/200], Loss: 1.3464, Accuracy: 61.57%\n",
            "Epoch [134/200], Loss: 1.3452, Accuracy: 61.57%\n",
            "Epoch [135/200], Loss: 1.3441, Accuracy: 61.57%\n",
            "Epoch [136/200], Loss: 1.3430, Accuracy: 61.57%\n",
            "Epoch [137/200], Loss: 1.3419, Accuracy: 61.57%\n",
            "Epoch [138/200], Loss: 1.3408, Accuracy: 61.57%\n",
            "Epoch [139/200], Loss: 1.3397, Accuracy: 61.65%\n",
            "Epoch [140/200], Loss: 1.3386, Accuracy: 61.57%\n",
            "Epoch [141/200], Loss: 1.3375, Accuracy: 61.57%\n",
            "Epoch [142/200], Loss: 1.3364, Accuracy: 61.57%\n",
            "Epoch [143/200], Loss: 1.3354, Accuracy: 61.65%\n",
            "Epoch [144/200], Loss: 1.3343, Accuracy: 61.80%\n",
            "Epoch [145/200], Loss: 1.3332, Accuracy: 61.73%\n",
            "Epoch [146/200], Loss: 1.3322, Accuracy: 61.80%\n",
            "Epoch [147/200], Loss: 1.3312, Accuracy: 61.73%\n",
            "Epoch [148/200], Loss: 1.3301, Accuracy: 61.73%\n",
            "Epoch [149/200], Loss: 1.3291, Accuracy: 61.73%\n",
            "Epoch [150/200], Loss: 1.3281, Accuracy: 61.88%\n",
            "Epoch [151/200], Loss: 1.3271, Accuracy: 61.88%\n",
            "Epoch [152/200], Loss: 1.3260, Accuracy: 61.73%\n",
            "Epoch [153/200], Loss: 1.3250, Accuracy: 61.80%\n",
            "Epoch [154/200], Loss: 1.3240, Accuracy: 61.88%\n",
            "Epoch [155/200], Loss: 1.3230, Accuracy: 61.96%\n",
            "Epoch [156/200], Loss: 1.3221, Accuracy: 61.96%\n",
            "Epoch [157/200], Loss: 1.3211, Accuracy: 61.96%\n",
            "Epoch [158/200], Loss: 1.3201, Accuracy: 61.96%\n",
            "Epoch [159/200], Loss: 1.3191, Accuracy: 62.04%\n",
            "Epoch [160/200], Loss: 1.3182, Accuracy: 62.04%\n",
            "Epoch [161/200], Loss: 1.3172, Accuracy: 62.12%\n",
            "Epoch [162/200], Loss: 1.3163, Accuracy: 62.20%\n",
            "Epoch [163/200], Loss: 1.3153, Accuracy: 62.20%\n",
            "Epoch [164/200], Loss: 1.3144, Accuracy: 62.20%\n",
            "Epoch [165/200], Loss: 1.3135, Accuracy: 62.20%\n",
            "Epoch [166/200], Loss: 1.3125, Accuracy: 62.12%\n",
            "Epoch [167/200], Loss: 1.3116, Accuracy: 62.04%\n",
            "Epoch [168/200], Loss: 1.3107, Accuracy: 62.04%\n",
            "Epoch [169/200], Loss: 1.3098, Accuracy: 61.96%\n",
            "Epoch [170/200], Loss: 1.3089, Accuracy: 61.88%\n",
            "Epoch [171/200], Loss: 1.3080, Accuracy: 61.96%\n",
            "Epoch [172/200], Loss: 1.3071, Accuracy: 62.04%\n",
            "Epoch [173/200], Loss: 1.3062, Accuracy: 62.20%\n",
            "Epoch [174/200], Loss: 1.3053, Accuracy: 62.20%\n",
            "Epoch [175/200], Loss: 1.3044, Accuracy: 62.27%\n",
            "Epoch [176/200], Loss: 1.3035, Accuracy: 62.35%\n",
            "Epoch [177/200], Loss: 1.3027, Accuracy: 62.35%\n",
            "Epoch [178/200], Loss: 1.3018, Accuracy: 62.51%\n",
            "Epoch [179/200], Loss: 1.3009, Accuracy: 62.43%\n",
            "Epoch [180/200], Loss: 1.3001, Accuracy: 62.43%\n",
            "Epoch [181/200], Loss: 1.2992, Accuracy: 62.35%\n",
            "Epoch [182/200], Loss: 1.2983, Accuracy: 62.27%\n",
            "Epoch [183/200], Loss: 1.2975, Accuracy: 62.27%\n",
            "Epoch [184/200], Loss: 1.2967, Accuracy: 62.20%\n",
            "Epoch [185/200], Loss: 1.2958, Accuracy: 62.20%\n",
            "Epoch [186/200], Loss: 1.2950, Accuracy: 62.20%\n",
            "Epoch [187/200], Loss: 1.2941, Accuracy: 62.20%\n",
            "Epoch [188/200], Loss: 1.2933, Accuracy: 62.20%\n",
            "Epoch [189/200], Loss: 1.2925, Accuracy: 62.20%\n",
            "Epoch [190/200], Loss: 1.2917, Accuracy: 62.27%\n",
            "Epoch [191/200], Loss: 1.2908, Accuracy: 62.27%\n",
            "Epoch [192/200], Loss: 1.2900, Accuracy: 62.27%\n",
            "Epoch [193/200], Loss: 1.2892, Accuracy: 62.27%\n",
            "Epoch [194/200], Loss: 1.2884, Accuracy: 62.27%\n",
            "Epoch [195/200], Loss: 1.2876, Accuracy: 62.35%\n",
            "Epoch [196/200], Loss: 1.2868, Accuracy: 62.43%\n",
            "Epoch [197/200], Loss: 1.2860, Accuracy: 62.51%\n",
            "Epoch [198/200], Loss: 1.2852, Accuracy: 62.43%\n",
            "Epoch [199/200], Loss: 1.2844, Accuracy: 62.35%\n",
            "Epoch [200/200], Loss: 1.2836, Accuracy: 62.20%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, Y_train_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_test_tensor)\n",
        "        _, predicted_val = torch.max(val_outputs, dim=1)\n",
        "        num_correct = (predicted_val == Y_test_tensor).sum().item()\n",
        "        accuracy = num_correct / Y_test_tensor.size(0) * 100\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL6K7xYGbAU_"
      },
      "source": [
        "Evaluation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "44R8qvnnbAw3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0  11   1   0   2]\n",
            " [  3 199  18   4  42]\n",
            " [  0  43 152   1   8]\n",
            " [  1  25  10   1   4]\n",
            " [  1  79   6   1  33]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        14\n",
            "           1       0.56      0.75      0.64       266\n",
            "           2       0.81      0.75      0.78       204\n",
            "           3       0.14      0.02      0.04        41\n",
            "           4       0.37      0.28      0.32       120\n",
            "\n",
            "    accuracy                           0.60       645\n",
            "   macro avg       0.38      0.36      0.35       645\n",
            "weighted avg       0.57      0.60      0.57       645\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "cm = confusion_matrix(Y_test_tensor.numpy(), predicted.numpy())\n",
        "print(cm)\n",
        "report = classification_report(Y_test_tensor.numpy(), predicted.numpy())\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
