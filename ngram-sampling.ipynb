{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8137129,"sourceType":"datasetVersion","datasetId":4810426}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook test des différentes méthodes de sampling pour le ngram\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-OyTF9d99C9","outputId":"b9dd57d6-b063-4896-fc55-9da4b26baff5","execution":{"iopub.status.busy":"2024-04-17T13:44:47.581519Z","iopub.execute_input":"2024-04-17T13:44:47.582157Z","iopub.status.idle":"2024-04-17T13:45:29.169053Z","shell.execute_reply.started":"2024-04-17T13:44:47.582114Z","shell.execute_reply":"2024-04-17T13:45:29.167591Z"}}},{"cell_type":"code","source":"!pip install kaggle\n!pip install tiktoken\n!pip install tensorflow\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-OyTF9d99C9","outputId":"b9dd57d6-b063-4896-fc55-9da4b26baff5","execution":{"iopub.status.busy":"2024-04-17T13:44:47.581519Z","iopub.execute_input":"2024-04-17T13:44:47.582157Z","iopub.status.idle":"2024-04-17T13:45:29.169053Z","shell.execute_reply.started":"2024-04-17T13:44:47.582114Z","shell.execute_reply":"2024-04-17T13:45:29.167591Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.8)\nRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.6)\nCollecting tiktoken\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\nDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.6.0\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.1.1\n    Uninstalling keras-3.1.1:\n      Successfully uninstalled keras-3.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U nltk==3.8.1.","metadata":{"execution":{"iopub.status.busy":"2024-04-17T14:17:16.369509Z","iopub.execute_input":"2024-04-17T14:17:16.370004Z","iopub.status.idle":"2024-04-17T14:17:30.312212Z","shell.execute_reply.started":"2024-04-17T14:17:16.369971Z","shell.execute_reply":"2024-04-17T14:17:30.310718Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting nltk==3.8.1.\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1.) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1.) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1.) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1.) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.4\n    Uninstalling nltk-3.4:\n      Successfully uninstalled nltk-3.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_czJ6QAuUdLo","outputId":"682a2d8d-3194-443f-c007-7ba777837700","execution":{"iopub.status.busy":"2024-04-16T13:21:28.349573Z","iopub.execute_input":"2024-04-16T13:21:28.350147Z","iopub.status.idle":"2024-04-16T13:21:28.757497Z","shell.execute_reply.started":"2024-04-16T13:21:28.350103Z","shell.execute_reply":"2024-04-16T13:21:28.755747Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"],"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport nltk\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tiktoken\nimport os\nimport math\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom spacy.lang.en.stop_words import STOP_WORDS as en_stop\nfrom collections import Counter, defaultdict\nimport torch\nfrom torch.nn import functional as F","metadata":{"id":"_APHPhEo-JyQ","execution":{"iopub.status.busy":"2024-04-17T16:14:17.913241Z","iopub.execute_input":"2024-04-17T16:14:17.914443Z","iopub.status.idle":"2024-04-17T16:14:35.568616Z","shell.execute_reply.started":"2024-04-17T16:14:17.914400Z","shell.execute_reply":"2024-04-17T16:14:35.567459Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-17 16:14:23.358197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-17 16:14:23.358340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-17 16:14:23.544279: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('wordnet')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfMONMMp-k07","outputId":"316ed4c3-17d6-4748-dd52-e2d2cb69baa7","execution":{"iopub.status.busy":"2024-04-17T16:14:35.839306Z","iopub.execute_input":"2024-04-17T16:14:35.839661Z","iopub.status.idle":"2024-04-17T16:14:35.846817Z","shell.execute_reply.started":"2024-04-17T16:14:35.839631Z","shell.execute_reply":"2024-04-17T16:14:35.845640Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"On fait un preprocessing des données","metadata":{"id":"pz2wCfJRl4ky"}},{"cell_type":"code","source":"skip_func = lambda x: x % 30 != 0\ndef split_data(data: pd.DataFrame):\n    \"\"\"\n    Split the data in a training and test set.\n    The training set is composed of 80% of the data.\n    The test set is composed of 20% of the data.\n    Useful links: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n    \"\"\"\n    train_set = data.sample(frac=0.8, random_state=0)\n    validation_set = data.drop(train_set.index)\n    return train_set, validation_set\ndef import_data_n_gram(file_path: str):\n    \"\"\"\n    Preprocess the data for the n-gram model.\n    The transformation effectuated are:\n    - Keep only the english songs and remove the language column\n    - Remove the NaN values\n    - Replace the square brackets by an empty string to remove the annotations and metadata\n    - Keep only the songs with a title length inferior to 50 characters\n    - Keep only the songs with a title composed of ASCII characters\n    - Concatenate the title and the lyrics in a single column\n    - Transform the text in lowercase\n    - Split the data in a training and validation set\n    Useful links: https://necromuralist.github.io/Neurotic-Networking/posts/nlp/n-gram-pre-processing/index.html\n    \"\"\"\n    data = pd.read_csv(file_path, usecols=['title', 'lyrics', 'language_cld3'], skiprows=lambda x: skip_func(x))\n    data = data.loc[data['language_cld3'] == 'en']\n    data = data.drop(columns=['language_cld3'])\n    data = data.dropna()\n    data['lyrics'] = data['lyrics'].str.replace(r\"\\[.*\\]\", \"\")\n    data = data[data['title'].apply(lambda x: len(x) < 50)]\n    data = data[data['title'].str.match(r'^[\\x00-\\x7F]*$')]\n    data['title_lyrics'] = data['title'] + ' ' + data['lyrics']\n    data[['title_lyrics', 'title', 'lyrics']] = data[['title_lyrics', 'title', 'lyrics']].apply(lambda x: x.str.lower())\n    train_set, validation_set = split_data(data)\n    return train_set, validation_set\nX_train, X_test = import_data_n_gram('/kaggle/input/dataset-genius/song_lyrics.csv')\nX_train = X_train['lyrics']\nX_test = X_test['lyrics']\n","metadata":{"id":"W1oSdJnmLDDc","execution":{"iopub.status.busy":"2024-04-17T16:14:58.159415Z","iopub.execute_input":"2024-04-17T16:14:58.159815Z","iopub.status.idle":"2024-04-17T16:17:43.586442Z","shell.execute_reply.started":"2024-04-17T16:14:58.159786Z","shell.execute_reply":"2024-04-17T16:17:43.585189Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXouOBU7pqo-","outputId":"7d1f0e67-61e7-4fdc-af4f-7d368fbff5ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":["27255    you know i can't take it anymore\\ntake me out ...\n","40372    [verse]\\nthe fire makes the city look so brigh...\n","45239    [verse 1]\\nlast night's party\\ndidn't go as pl...\n","11669    we crawled night and day through the tears and...\n","41631    this connection we got girl is something deepe...\n","                               ...                        \n","18084    [verse 1]\\nup so high, got so tied\\nyou got me...\n","18679    [round 1: aftershock]\\ni said you mans with my...\n","3439     [verse 1]\\ntoday what's day is night\\nsomeday ...\n","16479    [verse 1]\\nfirst light breaks in the morning s...\n","25463    [verse 1]\\nyou didn't spark no revolution\\nbet...\n","Name: lyrics, Length: 26034, dtype: object"]},"metadata":{}}]},{"cell_type":"markdown","source":"On réutilise les tokenizers du cours","metadata":{"id":"W82SXsMMl7U5"}},{"cell_type":"code","source":"#Define tokenizers from course\ndef lemma_tokenize(doc):\n    wnl = WordNetLemmatizer()\n    return [wnl.lemmatize(t) for t in word_tokenize(doc)]\n\ndef char_tokenize(doc):\n    return [char for char in doc]\n\ndef byte_tokenize(doc):\n    tokens = doc.encode(\"utf-8\")\n    tokens = list(map(int, tokens))\n    return [str(token) for token in tokens]\n\ndef gpt_tokenize(doc):\n    enc = tiktoken.encoding_for_model(\"gpt-4\")\n    tokens = enc.encode(doc)\n    return [str(token) for token in tokens]","metadata":{"id":"nK74ZLZkb3kz","execution":{"iopub.status.busy":"2024-04-17T16:18:20.711200Z","iopub.execute_input":"2024-04-17T16:18:20.712205Z","iopub.status.idle":"2024-04-17T16:18:20.721603Z","shell.execute_reply.started":"2024-04-17T16:18:20.712160Z","shell.execute_reply":"2024-04-17T16:18:20.720640Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Calcul des probabilités du ngram, elles nous serviront à générer les mots.","metadata":{"id":"afX3GuR9l-iC"}},{"cell_type":"code","source":"def tokenize(text):\n    \"\"\"Tokenize the input text.\"\"\"\n\n    return word_tokenize(text)\n\ndef count_ngrams(tokens, n):\n    \"\"\"Counts n-grams.\"\"\"\n\n    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n    return Counter(ngrams)\n\ndef calculate_ngram_probabilities(train_tokens, n, test_tokens, k=0.00001):\n    \"\"\"Calculates n-gram probabilities.\"\"\"\n\n    vocab = set(train_tokens)\n    V = len(vocab)\n    ngram_counts = count_ngrams(train_tokens, n)\n    n_minus_one_gram_counts = count_ngrams(train_tokens, n-1)\n    ngram_probabilities = defaultdict(float)\n\n    for ngram in ngram_counts:\n        prefix = ngram[:-1]\n        ngram_counts[ngram] += k\n        n_minus_one_gram_counts[prefix] += k\n        ngram_probabilities[ngram] = (ngram_counts[ngram] + k) / (n_minus_one_gram_counts[prefix] + k*V)\n\n    for i in range(len(test_tokens)-n+1):\n        ngram = tuple(test_tokens[i:i+n])\n        if ngram not in ngram_counts:\n            ngram_counts[ngram] = k\n            prefix = ngram[:-1]\n            if prefix not in n_minus_one_gram_counts:\n                n_minus_one_gram_counts[prefix] = k\n            ngram_probabilities[ngram] = (ngram_counts[ngram] + k) / (n_minus_one_gram_counts[prefix] + k*V)\n\n    return ngram_probabilities","metadata":{"id":"K1-a1bE1DwaK","execution":{"iopub.status.busy":"2024-04-17T16:18:22.369960Z","iopub.execute_input":"2024-04-17T16:18:22.370398Z","iopub.status.idle":"2024-04-17T16:18:22.381257Z","shell.execute_reply.started":"2024-04-17T16:18:22.370341Z","shell.execute_reply":"2024-04-17T16:18:22.380055Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Calcul des probabilites pour un 5-gram en tokenizant à l'aide de gpt","metadata":{"id":"lM64DA3WmEXz"}},{"cell_type":"code","source":"X_train_gpt = [gpt_tokenize(text) for text in X_train]\nX_test_gpt = [gpt_tokenize(text) for text in X_test]\nn = 5\n\ntrain_tokens = word_tokenize(\" \".join(X_train))\nvocab = set(train_tokens)\ntest_tokens = word_tokenize(\" \".join(X_test))\nngram_probabilities = calculate_ngram_probabilities(train_tokens, n, test_tokens, k=0.00001)\nngrams_counts = count_ngrams(train_tokens, n)","metadata":{"id":"aV5Q8sZVG4sf","execution":{"iopub.status.busy":"2024-04-17T13:31:08.385578Z","iopub.execute_input":"2024-04-17T13:31:08.386384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ngram_probabilities\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGYxrUpIKuIz","outputId":"daa2d492-cbf4-491e-c60e-a901abbb65fc","execution":{"iopub.status.busy":"2024-04-17T13:43:38.282803Z","iopub.execute_input":"2024-04-17T13:43:38.283778Z","iopub.status.idle":"2024-04-17T13:43:38.717070Z","shell.execute_reply.started":"2024-04-17T13:43:38.283726Z","shell.execute_reply":"2024-04-17T13:43:38.715213Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mngram_probabilities\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'ngram_probabilities' is not defined"],"ename":"NameError","evalue":"name 'ngram_probabilities' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Calcul de perplexité qui va nous permettre de définir la précision du modèle, plus on augmente le n, ,plus la perplexité du set d'entrainement diminue et celle du set de test augmente","metadata":{"id":"pO1dZAkkmOqz"}},{"cell_type":"code","source":"def calculate_perplexity(test_tokens, ngram_probabilities, n):\n    \"\"\"Calculates the perplexity of a test corpus given n-gram probabilities.\"\"\"\n    log_probability_sum = 0\n    ngram_count = 0\n\n    for i in range(len(test_tokens)-n+1):\n        ngram = tuple(test_tokens[i:i+n])\n        log_probability_sum += math.log2(ngram_probabilities[ngram])\n        ngram_count += 1\n\n    average_log_probability = -log_probability_sum / ngram_count\n    perplexity = math.pow(2, average_log_probability)\n\n    return perplexity","metadata":{"id":"RZhba82uYd_V","execution":{"iopub.status.busy":"2024-04-17T11:56:26.861604Z","iopub.execute_input":"2024-04-17T11:56:26.862372Z","iopub.status.idle":"2024-04-17T11:56:26.869568Z","shell.execute_reply.started":"2024-04-17T11:56:26.862338Z","shell.execute_reply":"2024-04-17T11:56:26.868536Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"calculate_perplexity(train_tokens, ngram_probabilities, n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Np-QjQALYiQU","outputId":"eac6e484-b1bd-401e-d193-a782b6adb427","execution":{"iopub.status.busy":"2024-04-17T11:56:28.736845Z","iopub.execute_input":"2024-04-17T11:56:28.737608Z","iopub.status.idle":"2024-04-17T11:56:44.827068Z","shell.execute_reply.started":"2024-04-17T11:56:28.737572Z","shell.execute_reply":"2024-04-17T11:56:44.825804Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"17.03522341729249"},"metadata":{}}]},{"cell_type":"code","source":"calculate_perplexity(test_tokens, ngram_probabilities, n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExR8nyMUZQzN","outputId":"099d4d08-ee03-49b7-aa2a-f59ac5204589","execution":{"iopub.status.busy":"2024-04-17T11:58:22.971217Z","iopub.execute_input":"2024-04-17T11:58:22.971728Z","iopub.status.idle":"2024-04-17T11:58:27.420773Z","shell.execute_reply.started":"2024-04-17T11:58:22.971693Z","shell.execute_reply":"2024-04-17T11:58:27.419372Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"2745.1074048237683"},"metadata":{}}]},{"cell_type":"markdown","source":"On va générer les mots à l'aide de plusieurs méthode de sampling, d'abord la méthode gloutonne vu en cours, ensuite la méthode top-k et enfin la méthode top-p","metadata":{"id":"ogXZyEjjmxDK"}},{"cell_type":"code","source":"def greedy_sampling(context, vocab, ngram_probabilities, n, max_length = 50):\n\n    sentence = []\n\n    if len(context) < (n-1):\n        print(\"len(context) < n\")\n        return sentence\n\n    context = context[-(n-1):]\n\n    for i in range(max_length):\n\n        probs = dict()\n\n        for v in vocab:\n\n            ngram = list(context)\n            ngram.append(v)\n            ngram = tuple(ngram)\n            probs[v] = ngram_probabilities[ngram]\n\n        best_token = max(probs, key=probs.get) # greedy\n        #print(best_v)\n        #print(probs[best_v])\n\n        if probs[best_token] == 0:\n            print(\"prob = 0\")\n            return sentence\n\n        sentence.append(best_token)\n        context = list(context)[1:]\n        context.append(best_token)\n        context = tuple(context)\n\n    return sentence\n    import random\n\ndef top_k_sampling(context, vocab, ngram_probabilities, n, k, max_length=50):\n    sentence = []\n\n    if len(context) < (n-1):\n        print(\"len(context) < n\")\n        return sentence\n\n    context = context[-(n-1):]\n\n    for i in range(max_length):\n        probs = dict()\n\n        for v in vocab:\n            ngram = list(context)\n            ngram.append(v)\n            ngram = tuple(ngram)\n            probs[v] = ngram_probabilities.get(ngram, 0)\n\n        top_k_tokens = sorted(probs, key=probs.get, reverse=True)[:k]\n\n        if sum(probs[token] for token in top_k_tokens) == 0:\n            print(\"All top-k probs are 0\")\n            return sentence\n\n        next_token = random.choices(top_k_tokens, weights=[probs[token] for token in top_k_tokens])[0]\n\n        sentence.append(next_token)\n        context = list(context)[1:]\n        context.append(next_token)\n        context = tuple(context)\n\n    return sentence\ndef top_p_sampling(context, vocab, ngram_probabilities, n, p, max_length=50):\n    sentence = []\n\n    if len(context) < (n-1):\n        print(\"len(context) < n\")\n        return sentence\n\n    context = context[-(n-1):]\n\n    for i in range(max_length):\n        probs = dict()\n\n        for v in vocab:\n            ngram = list(context)\n            ngram.append(v)\n            ngram = tuple(ngram)\n            probs[v] = ngram_probabilities.get(ngram, 0)\n\n        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n        cumulative_probs = []\n        cumulative_sum = 0\n\n        for token, prob in sorted_probs:\n            cumulative_sum += prob\n            cumulative_probs.append((token, cumulative_sum))\n            if cumulative_sum >= p:\n                break\n\n        if cumulative_sum == 0:\n            print(\"All probs are 0\")\n            return sentence\n\n        next_token = random.choices([t for t, _ in cumulative_probs],\n                                    weights=[prob for _, prob in cumulative_probs])[0]\n\n        sentence.append(next_token)\n        context = list(context)[1:]\n        context.append(next_token)\n        context = tuple(context)\n\n    return sentence\ndef top_k_top_p_sampling(context, vocab, ngram_probabilities, n, k, p, max_length=50):\n    sentence = []\n\n    if len(context) < (n-1):\n        print(\"len(context) < n\")\n        return sentence\n\n    context = context[-(n-1):]\n\n    for i in range(max_length):\n        probs = dict()\n\n        for v in vocab:\n            ngram = list(context)\n            ngram.append(v)\n            ngram = tuple(ngram)\n            probs[v] = ngram_probabilities.get(ngram, 0)\n\n        top_k_tokens = sorted(probs, key=probs.get, reverse=True)[:k]\n        \n        top_k_probs = {token: probs[token] for token in top_k_tokens}\n        \n        sorted_probs = sorted(top_k_probs.items(), key=lambda x: x[1], reverse=True)\n        cumulative_probs = []\n        cumulative_sum = 0\n\n        for token, prob in sorted_probs:\n            cumulative_sum += prob\n            cumulative_probs.append((token, cumulative_sum))\n            if cumulative_sum >= p:\n                break\n\n        if cumulative_sum == 0:\n            print(\"All top-k probs are 0\")\n            return sentence\n\n        next_token = random.choices([t for t, _ in cumulative_probs], \n                                    weights=[prob for _, prob in cumulative_probs])[0]\n\n        sentence.append(next_token)\n        context = list(context)[1:]\n        context.append(next_token)\n        context = tuple(context)\n\n    return sentence","metadata":{"id":"PLWrtsa-ZYdU","execution":{"iopub.status.busy":"2024-04-17T13:02:28.996239Z","iopub.execute_input":"2024-04-17T13:02:28.996704Z","iopub.status.idle":"2024-04-17T13:02:29.036871Z","shell.execute_reply.started":"2024-04-17T13:02:28.996658Z","shell.execute_reply":"2024-04-17T13:02:29.034919Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"context = ['you',\n              'believe',\n              'in',\n              'something',\n              'good']\n#context = ['the', 'one']\n#sentence = greedy_sampling(context, vocab, ngram_probabilities, n, max_length = 200)\nsentence = top_p_sampling(context, vocab, ngram_probabilities, n, 0.4, max_length = 200)\n#sentence = top_k_top_p_sampling(context, vocab, ngram_probabilities, n, 5,0.2, max_length = 200)\nprint(\" \".join(context) + \" \" +  \" \".join(sentence))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2bHeTcLZgRM","outputId":"9a88ed15-27bb-4225-ab83-27a2146a7d58","execution":{"iopub.status.busy":"2024-04-17T13:14:34.087733Z","iopub.execute_input":"2024-04-17T13:14:34.088325Z","iopub.status.idle":"2024-04-17T13:15:06.495347Z","shell.execute_reply.started":"2024-04-17T13:14:34.088263Z","shell.execute_reply":"2024-04-17T13:15:06.493588Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"All probs are 0\nyou believe in something good ? do you feel like movin ' movin' do you feel like movin ' movin' do you feel like movin ' movin' do you feel like movin ' movin' do you feel like yourself tonight ? ( do you feel ? ) do you wan na know why ? 'cause in bro's\n","output_type":"stream"}]}]}